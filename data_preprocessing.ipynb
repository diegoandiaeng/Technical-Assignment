{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b6bc700",
   "metadata": {},
   "source": [
    "UNIFIED DATA PREPROCESSING PIPELINE\n",
    "Loads NPZ and CSV data, performs train/test split and normalization.\n",
    "Saves all preprocessed data for use in training and analysis notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67918186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "UNIFIED DATA PREPROCESSING PIPELINE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"UNIFIED DATA PREPROCESSING PIPELINE\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1605d011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/5] Loading NPZ data...\n",
      "  ✓ Loaded NPZ: Data/Rpt0_N5000.npz\n",
      "    Total samples: 6315\n",
      "    Input shape: (6315, 5000, 9)\n",
      "    Output shape: (6315, 5000, 4)\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 1. LOAD NPZ DATA (PointCloud Data)\n",
    "# ==========================================\n",
    "print(\"\\n[1/5] Loading NPZ data...\")\n",
    "\n",
    "npz_path = 'Data/Rpt0_N5000.npz'\n",
    "data = np.load(npz_path, allow_pickle=True)\n",
    "\n",
    "# Extract all data\n",
    "all_input = data['a']      # (N_total, 5000, 9)\n",
    "all_output = data['b']     # (N_total, 5000, 4)\n",
    "all_names = data['c']      # Sample names\n",
    "\n",
    "print(f\"  ✓ Loaded NPZ: {npz_path}\")\n",
    "print(f\"    Total samples: {len(all_input)}\")\n",
    "print(f\"    Input shape: {all_input.shape}\")\n",
    "print(f\"    Output shape: {all_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bae5d3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2/5] Filtering diagonal load samples...\n",
      "  ✓ Diagonal samples: 2105 / 6315\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 2. FILTER DIAGONAL SAMPLES\n",
    "# ==========================================\n",
    "print(\"\\n[2/5] Filtering diagonal load samples...\")\n",
    "\n",
    "dia_mask = np.array(['dia' in str(name) for name in all_names])\n",
    "dia_input = all_input[dia_mask]      # (N_dia, 5000, 9)\n",
    "dia_output = all_output[dia_mask]    # (N_dia, 5000, 4)\n",
    "dia_names = all_names[dia_mask]\n",
    "\n",
    "print(f\"  ✓ Diagonal samples: {len(dia_input)} / {len(all_input)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "278883c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3/5] Loading and aligning CSV scalar data...\n",
      "  ✓ Successfully matched: 2105 samples\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 3. LOAD AND ALIGN CSV SCALARS\n",
    "# ==========================================\n",
    "print(\"\\n[3/5] Loading and aligning CSV scalar data...\")\n",
    "\n",
    "csv_path = 'Data/bracket_labels.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Create lookup dictionary\n",
    "csv_lookup = {}\n",
    "for idx, row in df.iterrows():\n",
    "    item_name = str(row['item_name'])\n",
    "    csv_lookup[item_name] = [\n",
    "        row['max_dia_stress(MPa)'],\n",
    "        row['mass(kg)'],\n",
    "        row['1st_mode_freq(Hz)']\n",
    "    ]\n",
    "\n",
    "# Match NPZ names with CSV\n",
    "dia_scalars = []\n",
    "valid_indices = []\n",
    "unmatched_names = []\n",
    "\n",
    "for idx, name in enumerate(dia_names):\n",
    "    name_str = str(name)\n",
    "    clean_name = name_str.replace('dia_', '').replace('hor_', '').replace('ver_', '')\n",
    "    \n",
    "    if clean_name in csv_lookup:\n",
    "        dia_scalars.append(csv_lookup[clean_name])\n",
    "        valid_indices.append(idx)\n",
    "    else:\n",
    "        unmatched_names.append(name_str)\n",
    "\n",
    "dia_scalars = np.array(dia_scalars, dtype=np.float32)\n",
    "valid_indices = np.array(valid_indices)\n",
    "\n",
    "# Filter to matched samples only\n",
    "dia_input = dia_input[valid_indices]\n",
    "dia_output = dia_output[valid_indices]\n",
    "dia_names = dia_names[valid_indices]\n",
    "\n",
    "print(f\"  ✓ Successfully matched: {len(dia_scalars)} samples\")\n",
    "if unmatched_names:\n",
    "    print(f\"  ⚠ Unmatched: {len(unmatched_names)} samples (excluded)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9d0a935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[4/5] Creating train/test split (80/20)...\n",
      "  ✓ Train set: 1684 samples\n",
      "  ✓ Test set: 421 samples\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 4. TRAIN/TEST SPLIT\n",
    "# ==========================================\n",
    "print(\"\\n[4/5] Creating train/test split (80/20)...\")\n",
    "\n",
    "train_idx, test_idx = train_test_split(\n",
    "    np.arange(len(dia_scalars)),\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split inputs\n",
    "train_input_full = dia_input[train_idx]  # (N_train, 5000, 9)\n",
    "test_input_full = dia_input[test_idx]    # (N_test, 5000, 9)\n",
    "\n",
    "# Extract XYZ coordinates only\n",
    "train_input_xyz = train_input_full[:, :, :3]  # (N_train, 5000, 3)\n",
    "test_input_xyz = test_input_full[:, :, :3]    # (N_test, 5000, 3)\n",
    "\n",
    "# Split outputs (field data)\n",
    "train_field_output = dia_output[train_idx]  # (N_train, 5000, 4)\n",
    "test_field_output = dia_output[test_idx]    # (N_test, 5000, 4)\n",
    "\n",
    "# Split scalars\n",
    "train_scalars = dia_scalars[train_idx]  # (N_train, 3)\n",
    "test_scalars = dia_scalars[test_idx]    # (N_test, 3)\n",
    "\n",
    "# Split names\n",
    "train_names = dia_names[train_idx]\n",
    "test_names = dia_names[test_idx]\n",
    "\n",
    "print(f\"  ✓ Train set: {len(train_idx)} samples\")\n",
    "print(f\"  ✓ Test set: {len(test_idx)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fb8c669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[5/5] Computing normalization statistics...\n",
      "  ✓ Field outputs normalized to [0, 1]\n",
      "  ✓ Scalar outputs normalized to [0, 1]\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 5. NORMALIZATION (using train statistics only)\n",
    "# ==========================================\n",
    "print(\"\\n[5/5] Computing normalization statistics...\")\n",
    "\n",
    "# --- FIELD OUTPUT NORMALIZATION ---\n",
    "field_min = train_field_output.min(axis=(0, 1))  # (4,)\n",
    "field_max = train_field_output.max(axis=(0, 1))  # (4,)\n",
    "\n",
    "train_field_norm = np.zeros_like(train_field_output, dtype=np.float32)\n",
    "test_field_norm = np.zeros_like(test_field_output, dtype=np.float32)\n",
    "\n",
    "for i in range(4):\n",
    "    train_field_norm[:, :, i] = (train_field_output[:, :, i] - field_min[i]) / (field_max[i] - field_min[i] + 1e-8)\n",
    "    test_field_norm[:, :, i] = (test_field_output[:, :, i] - field_min[i]) / (field_max[i] - field_min[i] + 1e-8)\n",
    "\n",
    "train_field_norm = np.clip(train_field_norm, 0, 1)\n",
    "test_field_norm = np.clip(test_field_norm, 0, 1)\n",
    "\n",
    "# --- SCALAR OUTPUT NORMALIZATION ---\n",
    "scalar_min = train_scalars.min(axis=0)  # (3,)\n",
    "scalar_max = train_scalars.max(axis=0)  # (3,)\n",
    "\n",
    "train_scalars_norm = (train_scalars - scalar_min) / (scalar_max - scalar_min + 1e-8)\n",
    "test_scalars_norm = (test_scalars - scalar_min) / (scalar_max - scalar_min + 1e-8)\n",
    "\n",
    "train_scalars_norm = np.clip(train_scalars_norm, 0, 1)\n",
    "test_scalars_norm = np.clip(test_scalars_norm, 0, 1)\n",
    "\n",
    "print(\"  ✓ Field outputs normalized to [0, 1]\")\n",
    "print(\"  ✓ Scalar outputs normalized to [0, 1]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "deff46ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SAVING PREPROCESSED DATA\n",
      "======================================================================\n",
      "\n",
      "✓ Saved to: Data/preprocessed/\n",
      "  - train_input_xyz.npy ((1684, 5000, 3))\n",
      "  - train_field_output.npy ((1684, 5000, 4))\n",
      "  - train_field_norm.npy ((1684, 5000, 4))\n",
      "  - train_scalars.npy ((1684, 3))\n",
      "  - train_scalars_norm.npy ((1684, 3))\n",
      "  - test_input_xyz.npy ((421, 5000, 3))\n",
      "  - test_field_output.npy ((421, 5000, 4))\n",
      "  - test_field_norm.npy ((421, 5000, 4))\n",
      "  - test_scalars.npy ((421, 3))\n",
      "  - test_scalars_norm.npy ((421, 3))\n",
      "  - train_names.npy, test_names.npy\n",
      "  - preprocess_info.pkl\n",
      "  - normalization_stats.npz\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# SAVE ALL PREPROCESSED DATA\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SAVING PREPROCESSED DATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "output_dir = 'Data/preprocessed'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Create preprocessing info dictionary\n",
    "preprocess_info = {\n",
    "    'train_idx': train_idx,\n",
    "    'test_idx': test_idx,\n",
    "    'field_min': field_min,\n",
    "    'field_max': field_max,\n",
    "    'scalar_min': scalar_min,\n",
    "    'scalar_max': scalar_max,\n",
    "    'scalar_columns': ['max_dia_stress(MPa)', 'mass(kg)', '1st_mode_freq(Hz)'],\n",
    "    'field_columns': ['ux (displacement)', 'uy (displacement)', 'uz (displacement)', 'von Mises stress'],\n",
    "    'n_train': len(train_idx),\n",
    "    'n_test': len(test_idx),\n",
    "    'n_points': 5000\n",
    "}\n",
    "\n",
    "# Save as pickle (easy to load)\n",
    "with open(f'{output_dir}/preprocess_info.pkl', 'wb') as f:\n",
    "    pickle.dump(preprocess_info, f)\n",
    "\n",
    "# Save train data\n",
    "np.save(f'{output_dir}/train_input_xyz.npy', train_input_xyz)\n",
    "np.save(f'{output_dir}/train_field_output.npy', train_field_output)\n",
    "np.save(f'{output_dir}/train_field_norm.npy', train_field_norm)\n",
    "np.save(f'{output_dir}/train_scalars.npy', train_scalars)\n",
    "np.save(f'{output_dir}/train_scalars_norm.npy', train_scalars_norm)\n",
    "\n",
    "# Save test data\n",
    "np.save(f'{output_dir}/test_input_xyz.npy', test_input_xyz)\n",
    "np.save(f'{output_dir}/test_field_output.npy', test_field_output)\n",
    "np.save(f'{output_dir}/test_field_norm.npy', test_field_norm)\n",
    "np.save(f'{output_dir}/test_scalars.npy', test_scalars)\n",
    "np.save(f'{output_dir}/test_scalars_norm.npy', test_scalars_norm)\n",
    "\n",
    "# Save names\n",
    "np.save(f'{output_dir}/train_names.npy', train_names, allow_pickle=True)\n",
    "np.save(f'{output_dir}/test_names.npy', test_names, allow_pickle=True)\n",
    "\n",
    "# Save normalization statistics separately\n",
    "np.savez(f'{output_dir}/normalization_stats.npz',\n",
    "        field_min=field_min, field_max=field_max,\n",
    "        scalar_min=scalar_min, scalar_max=scalar_max)\n",
    "\n",
    "print(f\"\\n✓ Saved to: {output_dir}/\")\n",
    "print(f\"  - train_input_xyz.npy ({train_input_xyz.shape})\")\n",
    "print(f\"  - train_field_output.npy ({train_field_output.shape})\")\n",
    "print(f\"  - train_field_norm.npy ({train_field_norm.shape})\")\n",
    "print(f\"  - train_scalars.npy ({train_scalars.shape})\")\n",
    "print(f\"  - train_scalars_norm.npy ({train_scalars_norm.shape})\")\n",
    "print(f\"  - test_input_xyz.npy ({test_input_xyz.shape})\")\n",
    "print(f\"  - test_field_output.npy ({test_field_output.shape})\")\n",
    "print(f\"  - test_field_norm.npy ({test_field_norm.shape})\")\n",
    "print(f\"  - test_scalars.npy ({test_scalars.shape})\")\n",
    "print(f\"  - test_scalars_norm.npy ({test_scalars_norm.shape})\")\n",
    "print(f\"  - train_names.npy, test_names.npy\")\n",
    "print(f\"  - preprocess_info.pkl\")\n",
    "print(f\"  - normalization_stats.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bf242c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DATA SUMMARY\n",
      "======================================================================\n",
      "\n",
      "SAMPLE COUNTS:\n",
      "  Training: 1684\n",
      "  Testing:  421\n",
      "  Total:    2105\n",
      "\n",
      "FIELD OUTPUT STATISTICS (ORIGINAL SCALE):\n",
      "\n",
      "  ux (disp):\n",
      "    Min:     -0.737986\n",
      "    Max:      0.113362\n",
      "    Range:     0.851348\n",
      "\n",
      "  uy (disp):\n",
      "    Min:     -0.276773\n",
      "    Max:      0.274422\n",
      "    Range:     0.551195\n",
      "\n",
      "  uz (disp):\n",
      "    Min:     -0.039458\n",
      "    Max:      0.743882\n",
      "    Range:     0.783340\n",
      "\n",
      "  von Mises stress:\n",
      "    Min:      0.000000\n",
      "    Max:    700.386475\n",
      "    Range:   700.386475\n",
      "\n",
      "SCALAR OUTPUT STATISTICS (ORIGINAL SCALE):\n",
      "\n",
      "  Max Stress (MPa):\n",
      "    Min:    286.685944\n",
      "    Max:   1067.449707\n",
      "    Mean (train):   543.181885\n",
      "    Std (train):    118.700417\n",
      "\n",
      "  Mass (kg):\n",
      "    Min:      0.561002\n",
      "    Max:      2.407380\n",
      "    Mean (train):     1.234920\n",
      "    Std (train):      0.381467\n",
      "\n",
      "  1st Freq (Hz):\n",
      "    Min:    752.470276\n",
      "    Max:   6850.030762\n",
      "    Mean (train):  3221.244141\n",
      "    Std (train):   1404.028076\n",
      "\n",
      "======================================================================\n",
      "✅ PREPROCESSING COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "Usage in training notebooks:\n",
      "  from data_loader import load_preprocessed_data\n",
      "  data = load_preprocessed_data()\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# PRINT SUMMARY STATISTICS\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DATA SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nSAMPLE COUNTS:\")\n",
    "print(f\"  Training: {len(train_idx)}\")\n",
    "print(f\"  Testing:  {len(test_idx)}\")\n",
    "print(f\"  Total:    {len(train_idx) + len(test_idx)}\")\n",
    "\n",
    "print(f\"\\nFIELD OUTPUT STATISTICS (ORIGINAL SCALE):\")\n",
    "field_names = ['ux (disp)', 'uy (disp)', 'uz (disp)', 'von Mises stress']\n",
    "for i, name in enumerate(field_names):\n",
    "    print(f\"\\n  {name}:\")\n",
    "    print(f\"    Min:  {field_min[i]:12.6f}\")\n",
    "    print(f\"    Max:  {field_max[i]:12.6f}\")\n",
    "    print(f\"    Range: {field_max[i] - field_min[i]:12.6f}\")\n",
    "\n",
    "print(f\"\\nSCALAR OUTPUT STATISTICS (ORIGINAL SCALE):\")\n",
    "scalar_names = ['Max Stress (MPa)', 'Mass (kg)', '1st Freq (Hz)']\n",
    "for i, name in enumerate(scalar_names):\n",
    "    print(f\"\\n  {name}:\")\n",
    "    print(f\"    Min:  {scalar_min[i]:12.6f}\")\n",
    "    print(f\"    Max:  {scalar_max[i]:12.6f}\")\n",
    "    print(f\"    Mean (train): {train_scalars[:, i].mean():12.6f}\")\n",
    "    print(f\"    Std (train):  {train_scalars[:, i].std():12.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"✅ PREPROCESSING COMPLETE!\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nUsage in training notebooks:\")\n",
    "print(\"  from data_loader import load_preprocessed_data\")\n",
    "print(\"  data = load_preprocessed_data()\")\n",
    "print(\"=\" * 70 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
